{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "see README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7776,
     "status": "ok",
     "timestamp": 1526105952629,
     "user": {
      "displayName": "Chengwei Zhang",
      "photoUrl": "//lh5.googleusercontent.com/-FK2ckwmh6mM/AAAAAAAAAAI/AAAAAAAAPLw/SX9b1QAzJ5g/s50-c-k-no/photo.jpg",
      "userId": "114808171854651597062"
     },
     "user_tz": -480
    },
    "id": "1wsDPx682A7H",
    "outputId": "88ee055c-b674-4a81-c869-8401551f09d7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "\n",
    "\n",
    "import datetime\n",
    "\n",
    "import editdistance\n",
    "import numpy as np\n",
    "\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.layers import Reshape, Lambda\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing import image\n",
    "import keras.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Xnj6R_OE3yMl"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "np.random.seed(55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mM0eHsJO5bP6"
   },
   "outputs": [],
   "source": [
    "import generator_text_image as fake\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify what we have with existing generator\n",
    "def lets_understand_original_generator():\n",
    "    \n",
    "    img_w = 128\n",
    "    img_h = 64\n",
    "    pool_size = 2\n",
    "    words_per_epoch = 16000\n",
    "    val_split = 0.2\n",
    "    val_words = int(words_per_epoch * (val_split))\n",
    "    minibatch_size = 32\n",
    "    \n",
    "    print(\"INPUT to generator:\")\n",
    "    print(\"-------------------\")\n",
    "    print()\n",
    "\n",
    "\n",
    "    fdir = os.path.dirname(get_file('wordlists.tgz',\n",
    "                                    origin='http://www.mythic-ai.com/datasets/wordlists.tgz', untar=True))\n",
    "\n",
    "    print(\"img_w\", img_w)\n",
    "    print(\"img_h\", img_h)\n",
    "    print(\"pool_size\", pool_size)\n",
    "    print(\"words_per_epoch\", words_per_epoch)\n",
    "    print(\"val_words\", val_words)\n",
    "    img_gen = fake.TextImageGenerator(monogram_file=os.path.join(fdir, 'wordlist_mono_clean.txt'),\n",
    "                                 bigram_file=os.path.join(fdir, 'wordlist_bi_clean.txt'),\n",
    "                                 minibatch_size=minibatch_size,\n",
    "                                 img_w=img_w,\n",
    "                                 img_h=img_h,\n",
    "                                 downsample_factor=(pool_size ** 2),\n",
    "                                 val_split=words_per_epoch - val_words\n",
    "                                 )\n",
    "    \n",
    "    print()\n",
    "    print(\"OUTPUT from generator:\")\n",
    "    print(\"----------------------\")\n",
    "    print()\n",
    "    print(\"img_gen.get_output_size()\", img_gen.get_output_size())\n",
    "    print(\"img_gen.absolute_max_string_len\", img_gen.absolute_max_string_len)\n",
    "    img_gen.on_train_begin()\n",
    "    \n",
    "    #temp = img_gen.next_train() # output is dummy data according to comments\n",
    "    \n",
    "    #for i in temp:\n",
    "    i =  img_gen.get_batch(1, 32, train=True)\n",
    "    temp_input, temp_output_is_dummy = i\n",
    "\n",
    "    print()\n",
    "    print('this corresponds to cnn_rnn_model Input(name=the_input')\n",
    "    print(\"the_input is a set of images \", temp_input['the_input'].shape)\n",
    "\n",
    "    print()\n",
    "    print(\"source_str for visualization\", temp_input['source_str'][:8])\n",
    "\n",
    "    print()\n",
    "    print(\"the next are inputs to the model for ctc calculation\")\n",
    "    print()\n",
    "    print(\"this corresponds to cnn_rnn_model Input(name='the_labels'\")\n",
    "    print(\"the_labels are char index from a, ex:t=19\")\n",
    "    print(temp_input['the_labels'][:2])\n",
    "\n",
    "    print()\n",
    "    print(\"this corresponds to cnn_rnn_model Input(name='input_length'\")\n",
    "    print('input_length of ctc is 2 short, see ctc_drop_first_2')\n",
    "    print(temp_input['input_length'][:2])\n",
    "\n",
    "    print()\n",
    "    print(\"this corresponds to cnn_rnn_model Input(name='label_length'\")\n",
    "    print('label_length is number of chars in word')\n",
    "    print(temp_input['label_length'][:8])\n",
    "\n",
    "   #     break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lets_understand_original_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IAM based generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import numpy as np\n",
    "import generator_iam_words as IAM\n",
    "import generator_german_words as GER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify what we have with existing generator\n",
    "def lets_understand_IAM():\n",
    "    \n",
    "    img_w = 128\n",
    "    img_h = 64\n",
    "    pool_size = 2\n",
    "    words_per_epoch = 16000\n",
    "    val_split = 0.2\n",
    "    val_words = int(words_per_epoch * (val_split))\n",
    "    minibatch_size = 32\n",
    "    \n",
    "    print(\"INPUT to generator:\")\n",
    "    print(\"-------------------\")\n",
    "    print()\n",
    "    \n",
    "   \n",
    "    print(\"img_w\", img_w)\n",
    "    print(\"img_h\", img_h)\n",
    "    print(\"pool_size\", pool_size)\n",
    "    print(\"words_per_epoch\", words_per_epoch)\n",
    "    print(\"val_words\", val_words)\n",
    "    g = IAM.IAM_Word_Generator(minibatch_size = 32, img_w = 128, img_h = 64, downsample_factor=4, absolute_max_string_len=16)\n",
    " \n",
    "    g.on_train_begin()\n",
    "    x = g.get_batch(-1, 32, True)\n",
    "  \n",
    "    return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = lets_understand_IAM()\n",
    "input, output = x\n",
    "\n",
    "the_input = input[\"the_input\"]\n",
    "print(the_input.shape)\n",
    "\n",
    "print(input[\"source_str\"])\n",
    "\n",
    "print(input['the_labels'])\n",
    "\n",
    "print(input['input_length'])\n",
    "\n",
    "print(input['label_length'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three Generators Side by Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def side_by_side():\n",
    "    img_w = 128\n",
    "    img_h = 64\n",
    "    pool_size = 2\n",
    "    words_per_epoch = 16000\n",
    "    val_split = 0.2\n",
    "    val_words = int(words_per_epoch * (val_split))\n",
    "    minibatch_size = 32\n",
    "    \n",
    "    fdir = os.path.dirname(get_file('wordlists.tgz',\n",
    "                                    origin='http://www.mythic-ai.com/datasets/wordlists.tgz', untar=True))\n",
    "\n",
    "    # original\n",
    "    img_gen = fake.TextImageGenerator(monogram_file=os.path.join(fdir, 'wordlist_mono_clean.txt'),\n",
    "                                 bigram_file=os.path.join(fdir, 'wordlist_bi_clean.txt'),\n",
    "                                 minibatch_size=minibatch_size,\n",
    "                                 img_w=img_w,\n",
    "                                 img_h=img_h,\n",
    "                                 downsample_factor=(pool_size ** 2),\n",
    "                                 val_split=words_per_epoch - val_words\n",
    "                                 )\n",
    "    img_gen.on_train_begin()\n",
    "    orig_batch =  img_gen.get_batch(1, 32, train=True)\n",
    "    \n",
    "    # IAM based\n",
    "    iam_gen = IAM.IAM_Word_Generator(minibatch_size = 32, img_w = 128, img_h = 64, downsample_factor=4, absolute_max_string_len=16)\n",
    "    iam_gen.on_train_begin()\n",
    "    iam_batch = iam_gen.get_batch(iam_gen.words_id_text_list, 32, True)\n",
    "    \n",
    "    # German Chruch record\n",
    "    ger_gen = GER.German_Word_Generator(minibatch_size = 32, img_w = 128, img_h = 64, downsample_factor=4, absolute_max_string_len=16)\n",
    "    ger_gen.on_train_begin()\n",
    "    ger_batch = ger_gen.get_batch(ger_gen.words_id_text_list, 32, True)\n",
    "    \n",
    "    print(\"len of get_batch, orig vs IAM vs ger,\", \n",
    "          len(orig_batch), \"vs\", \n",
    "          len(iam_batch), \"vs\", len(ger_batch))\n",
    "   \n",
    "    input_orig, output_orig = orig_batch\n",
    "    input_iam, output_iam = iam_batch\n",
    "    input_ger, output_ger = ger_batch\n",
    "\n",
    "    the_input_orig = input_orig[\"the_input\"]\n",
    "    the_input_iam = input_iam[\"the_input\"]\n",
    "    the_input_ger = input_ger[\"the_input\"]\n",
    "    \n",
    "    print(\"the_input ie: image\", the_input_orig.shape, \"vs\", the_input_iam.shape, \"vs\", the_input_ger.shape)\n",
    "    print(\"image[0,0,0,0]\")\n",
    "\n",
    "    print(\"source_str, first 5\", input_orig[\"source_str\"][:5], \"vs\", input_iam[\"source_str\"][:5], \"vs\", input_ger[\"source_str\"][:5])\n",
    "\n",
    "    print(\"the_labels, first 2\", input_orig['the_labels'][:2], \"vs\", input_iam['the_labels'][:2], \"vs\", input_ger['the_labels'][:2])\n",
    "\n",
    "    print(\"input_length, first 2\", input_orig['input_length'][:2], \"vs\", input_iam['input_length'][:2], \"vs\", input_ger['input_length'][:2])\n",
    "\n",
    "    print(\"label_length, first 2\", input_orig['label_length'], \"vs\", input_iam['label_length'], \"vs\", input_ger['label_length'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on_train_begin\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "on_train_begin\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "shape (128, 64, 1)\n",
      "len of get_batch, orig vs IAM vs ger, 2 vs 2 vs 2\n",
      "the_input ie: image (32, 128, 64, 1) vs (32, 128, 64, 1) vs (32, 128, 64, 1)\n",
      "image[0,0,0,0]\n",
      "source_str, first 5 ['rmp', 'of', 'arpt', 'and', 'axon'] vs ['Iain' 'of' 'Wales' 'to' 'but'] vs ['Bürger' 'ff' 'Josef' 'Q' 'Ogris']\n",
      "the_labels, first 2 [[17. 12. 15. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [14.  5. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]] vs [[ 8 26 34 39 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      " [40 31 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]] vs [[ 1 59 43 32 30 43 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      " [31 31 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]]\n",
      "input_length, first 2 [[30.]\n",
      " [30.]] vs [[30.]\n",
      " [30.]] vs [[30.]\n",
      " [30.]]\n",
      "label_length, first 2 [[3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] vs [[4.]\n",
      " [2.]\n",
      " [5.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [8.]\n",
      " [2.]\n",
      " [3.]\n",
      " [7.]\n",
      " [3.]\n",
      " [5.]\n",
      " [5.]\n",
      " [2.]\n",
      " [4.]\n",
      " [6.]\n",
      " [1.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [5.]\n",
      " [5.]] vs [[6.]\n",
      " [2.]\n",
      " [5.]\n",
      " [1.]\n",
      " [5.]\n",
      " [1.]\n",
      " [4.]\n",
      " [5.]\n",
      " [9.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [5.]\n",
      " [6.]\n",
      " [1.]\n",
      " [1.]\n",
      " [5.]\n",
      " [6.]\n",
      " [1.]\n",
      " [1.]\n",
      " [3.]\n",
      " [1.]\n",
      " [6.]\n",
      " [1.]\n",
      " [6.]\n",
      " [1.]\n",
      " [1.]\n",
      " [7.]]\n"
     ]
    }
   ],
   "source": [
    "side_by_side()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "image_ocr.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
