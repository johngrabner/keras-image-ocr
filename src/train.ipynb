{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "see README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7776,
     "status": "ok",
     "timestamp": 1526105952629,
     "user": {
      "displayName": "Chengwei Zhang",
      "photoUrl": "//lh5.googleusercontent.com/-FK2ckwmh6mM/AAAAAAAAAAI/AAAAAAAAPLw/SX9b1QAzJ5g/s50-c-k-no/photo.jpg",
      "userId": "114808171854651597062"
     },
     "user_tz": -480
    },
    "id": "1wsDPx682A7H",
    "outputId": "88ee055c-b674-4a81-c869-8401551f09d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "\n",
    "\n",
    "import datetime\n",
    "\n",
    "import editdistance\n",
    "import numpy as np\n",
    "\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.layers import Reshape, Lambda\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing import image\n",
    "import keras.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Xnj6R_OE3yMl"
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = 'image_ocr'\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mM0eHsJO5bP6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code under development ...\n",
      "click debug in visual studio code\n"
     ]
    }
   ],
   "source": [
    "# Text_Image is the original generator. It creates images programaticaly. \n",
    "# Script_Image takes handwritten words from the IAM database.\n",
    "generator_choice =  \"Script_Image\" # \"Text_Image\" #\n",
    "import generator_text_image as GTI\n",
    "import generator_iam_words as IAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "HBLwtxF35f0c"
   },
   "outputs": [],
   "source": [
    "import ctc_drop_first_2\n",
    "import cnn_rnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "YOAsKfXm5jP1"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# For a real OCR application, this should be beam search with a dictionary\n",
    "# and language model.  For this example, best path is sufficient.\n",
    "\n",
    "def decode_batch(test_func, word_batch):\n",
    "    out = test_func([word_batch])[0]\n",
    "    ret = []\n",
    "    for j in range(out.shape[0]):\n",
    "        out_best = list(np.argmax(out[j, 2:], 1))\n",
    "        out_best = [k for k, g in itertools.groupby(out_best)]\n",
    "        if generator_choice ==  \"Script_Image\":\n",
    "            outstr = IAM.labels_to_text(out_best)\n",
    "        else:\n",
    "            outstr = GTI.labels_to_text(out_best)\n",
    "        ret.append(outstr)\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KcGtIsLF5leW"
   },
   "outputs": [],
   "source": [
    "class VizCallback(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, run_name, test_func, text_img_gen, num_display_words=6):\n",
    "        self.test_func = test_func\n",
    "        self.output_dir = os.path.join(\n",
    "            OUTPUT_DIR, run_name)\n",
    "        self.text_img_gen = text_img_gen\n",
    "        self.num_display_words = num_display_words\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "    def show_edit_distance(self, num):\n",
    "        num_left = num\n",
    "        mean_norm_ed = 0.0\n",
    "        mean_ed = 0.0\n",
    "        while num_left > 0:\n",
    "            word_batch = next(self.text_img_gen)[0]\n",
    "            num_proc = min(word_batch['the_input'].shape[0], num_left)\n",
    "            decoded_res = decode_batch(self.test_func, word_batch['the_input'][0:num_proc])\n",
    "            for j in range(num_proc):\n",
    "                edit_dist = editdistance.eval(decoded_res[j], word_batch['source_str'][j])\n",
    "                mean_ed += float(edit_dist)\n",
    "                mean_norm_ed += float(edit_dist) / len(word_batch['source_str'][j])\n",
    "            num_left -= num_proc\n",
    "        mean_norm_ed = mean_norm_ed / num\n",
    "        mean_ed = mean_ed / num\n",
    "        print('\\nOut of %d samples:  Mean edit distance: %.3f Mean normalized edit distance: %0.3f'\n",
    "              % (num, mean_ed, mean_norm_ed))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.model.save_weights(os.path.join(self.output_dir, 'weights%02d.h5' % (epoch)))\n",
    "        self.show_edit_distance(256)\n",
    "        word_batch = next(self.text_img_gen)[0]\n",
    "        res = decode_batch(self.test_func, word_batch['the_input'][0:self.num_display_words])\n",
    "        if word_batch['the_input'][0].shape[0] < 256:\n",
    "            cols = 2\n",
    "        else:\n",
    "            cols = 1\n",
    "        for i in range(self.num_display_words):\n",
    "            plt.subplot(self.num_display_words // cols, cols, i + 1)\n",
    "            if K.image_data_format() == 'channels_first':\n",
    "                the_input = word_batch['the_input'][i, 0, :, :]\n",
    "            else:\n",
    "                the_input = word_batch['the_input'][i, :, :, 0]\n",
    "            plt.imshow(the_input.T, cmap='Greys_r')\n",
    "            plt.xlabel('Truth = \\'%s\\'\\nDecoded = \\'%s\\'' % (word_batch['source_str'][i], res[i]))\n",
    "        fig = pylab.gcf()\n",
    "        fig.set_size_inches(10, 13)\n",
    "        plt.savefig(os.path.join(self.output_dir, 'e%02d.png' % (epoch)))\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "eC4vEfcQ5oOq"
   },
   "outputs": [],
   "source": [
    "img_gen = None\n",
    "def train(run_name, start_epoch, stop_epoch, img_w):\n",
    "    global img_gen\n",
    "    img_h = 64\n",
    "    \n",
    "    # Input Parameters\n",
    "\n",
    "    words_per_epoch = 16000\n",
    "    val_split = 0.2\n",
    "    \n",
    "    # 16000 * 0.2 = 3200\n",
    "    val_words = int(words_per_epoch * (val_split))\n",
    "\n",
    "    # Network parameters\n",
    "    minibatch_size = 32\n",
    "    \n",
    "    \n",
    "    # (16000 - 3200) // 32 = 400\n",
    "    steps_per_epoch = (words_per_epoch - val_words) // minibatch_size\n",
    "\n",
    "\n",
    "    if generator_choice == \"Text_Image\":\n",
    "        fdir = os.path.dirname(get_file('wordlists.tgz',\n",
    "                                        origin='http://www.mythic-ai.com/datasets/wordlists.tgz', untar=True))\n",
    "\n",
    "        img_gen = GTI.TextImageGenerator(monogram_file=os.path.join(fdir, 'wordlist_mono_clean.txt'),\n",
    "                                     bigram_file=os.path.join(fdir, 'wordlist_bi_clean.txt'),\n",
    "                                     minibatch_size=minibatch_size,\n",
    "                                     img_w=img_w,\n",
    "                                     img_h=img_h,\n",
    "                                     downsample_factor=(pool_size ** 2),\n",
    "                                     val_split=words_per_epoch - val_words\n",
    "                                     )\n",
    "    elif generator_choice == \"Script_Image\":\n",
    "        img_gen = IAM.IAM_Word_Generator(minibatch_size = 32, img_w = img_w, img_h = img_h, downsample_factor=4, absolute_max_string_len=16)\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "    model, input_data, y_pred = cnn_rnn_model.make_model(img_w, img_h, img_gen.get_output_size(), img_gen.absolute_max_string_len)\n",
    "    \n",
    "    \n",
    "    model.summary() # print summary of model\n",
    "\n",
    "    # clipnorm seems to speeds up convergence\n",
    "    sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
    "    \n",
    "    # the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n",
    "    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=sgd)\n",
    "    if start_epoch > 0:\n",
    "        weight_file = os.path.join(OUTPUT_DIR, os.path.join(run_name, 'weights%02d.h5' % (start_epoch - 1)))\n",
    "        model.load_weights(weight_file)\n",
    "    # captures output of softmax so we can decode the output during visualization\n",
    "    test_func = K.function([input_data], [y_pred])\n",
    "\n",
    "    viz_cb = VizCallback(run_name, test_func, img_gen.next_val())\n",
    "\n",
    "    model.fit_generator(generator=img_gen.next_train(),\n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        epochs=stop_epoch,\n",
    "                        validation_data=img_gen.next_val(),\n",
    "                        validation_steps=val_words // minibatch_size,\n",
    "                        callbacks=[viz_cb, img_gen],\n",
    "                        initial_epoch=start_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "13kO-MxI7XWb"
   },
   "source": [
    "### Start the training\n",
    "\n",
    "1080ti run times: \n",
    "- 12 minutes for 20/20 epoch\n",
    "- 25 minutes for 25/25 epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2094
    },
    "colab_type": "code",
    "id": "YRGglF5L5qzn",
    "outputId": "541e795b-80d1-49e6-9c9f-e2014690f4a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Tensor(\"the_labels:0\", shape=(?, 16), dtype=float32)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4249: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4229: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          (None, 128, 64, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 128, 64, 16)  160         the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max1 (MaxPooling2D)             (None, 64, 32, 16)   0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 64, 32, 16)   2320        max1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "max2 (MaxPooling2D)             (None, 32, 16, 16)   0           conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 32, 256)      0           max2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 32, 32)       8224        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru1 (GRU)                      (None, 32, 512)      837120      dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru1_b (GRU)                    (None, 32, 512)      837120      dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 512)      0           gru1[0][0]                       \n",
      "                                                                 gru1_b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru2 (GRU)                      (None, 32, 512)      1574400     add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gru2_b (GRU)                    (None, 32, 512)      1574400     add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 1024)     0           gru2[0][0]                       \n",
      "                                                                 gru2_b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 32, 54)       55350       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Activation)            (None, 32, 54)       0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         (None, 16)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           softmax[0][0]                    \n",
      "                                                                 the_labels[0][0]                 \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 4,889,094\n",
      "Trainable params: 4,889,094\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "on_train_begin\n",
      "Epoch 1/20\n",
      "on_epoch_begin\n",
      "400/400 [==============================] - 36s 91ms/step - loss: 12.0528 - val_loss: 9.3673\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.516 Mean normalized edit distance: 0.748\n",
      "Epoch 2/20\n",
      "on_epoch_begin\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 8.4485 - val_loss: 7.6285\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 1.961 Mean normalized edit distance: 0.595\n",
      "Epoch 3/20\n",
      "on_epoch_begin\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 7.1127 - val_loss: 6.6208\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 1.801 Mean normalized edit distance: 0.506\n",
      "Epoch 4/20\n",
      "on_epoch_begin\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 6.1750 - val_loss: 6.3484\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 1.645 Mean normalized edit distance: 0.468\n",
      "Epoch 5/20\n",
      "on_epoch_begin\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 5.6162 - val_loss: 5.6134\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 1.559 Mean normalized edit distance: 0.427\n",
      "Epoch 6/20\n",
      "on_epoch_begin\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 5.2131 - val_loss: 5.2175\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 1.410 Mean normalized edit distance: 0.397\n",
      "Epoch 7/20\n",
      "on_epoch_begin\n",
      "400/400 [==============================] - 34s 84ms/step - loss: 4.8409 - val_loss: 5.2057\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 1.367 Mean normalized edit distance: 0.365\n",
      "Epoch 8/20\n",
      "on_epoch_begin\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 4.4655 - val_loss: 4.9789\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 1.391 Mean normalized edit distance: 0.339\n",
      "Epoch 9/20\n",
      "on_epoch_begin\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 4.2295 - val_loss: 4.9003\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 1.289 Mean normalized edit distance: 0.345\n",
      "Epoch 10/20\n",
      "on_epoch_begin\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 4.0062 - val_loss: 4.6674\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 1.141 Mean normalized edit distance: 0.308\n",
      "Epoch 11/20\n",
      "on_epoch_begin\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 3.7787 - val_loss: 4.3965\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 1.078 Mean normalized edit distance: 0.283\n",
      "Epoch 12/20\n",
      "on_epoch_begin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 32s 80ms/step - loss: 3.6103 - val_loss: 4.3972\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 1.191 Mean normalized edit distance: 0.328\n",
      "Epoch 13/20\n",
      "on_epoch_begin\n",
      "400/400 [==============================] - 32s 80ms/step - loss: 3.4918 - val_loss: 4.6495\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 1.137 Mean normalized edit distance: 0.295\n",
      "Epoch 14/20\n",
      "on_epoch_begin\n",
      "400/400 [==============================] - 32s 80ms/step - loss: 3.3663 - val_loss: 4.2525\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 0.988 Mean normalized edit distance: 0.267\n",
      "Epoch 15/20\n",
      "on_epoch_begin\n",
      "400/400 [==============================] - 32s 80ms/step - loss: 3.1356 - val_loss: 4.3775\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 1.324 Mean normalized edit distance: 0.325\n",
      "Epoch 16/20\n",
      "on_epoch_begin\n",
      "400/400 [==============================] - 32s 80ms/step - loss: 2.9794 - val_loss: 4.3755\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 1.328 Mean normalized edit distance: 0.315\n",
      "Epoch 17/20\n",
      "on_epoch_begin\n",
      "400/400 [==============================] - 32s 81ms/step - loss: 2.8221 - val_loss: 4.0031\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 0.961 Mean normalized edit distance: 0.246\n",
      "Epoch 18/20\n",
      "on_epoch_begin\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 2.7617 - val_loss: 4.0229\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 1.223 Mean normalized edit distance: 0.303\n",
      "Epoch 19/20\n",
      "on_epoch_begin\n",
      "400/400 [==============================] - 34s 84ms/step - loss: 2.6310 - val_loss: 3.9263\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 1.035 Mean normalized edit distance: 0.318\n",
      "Epoch 20/20\n",
      "on_epoch_begin\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 2.5478 - val_loss: 3.9929\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 0.879 Mean normalized edit distance: 0.234\n",
      "Tensor(\"the_labels_1:0\", shape=(?, 16), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          (None, 512, 64, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 512, 64, 16)  160         the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max1 (MaxPooling2D)             (None, 256, 32, 16)  0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 256, 32, 16)  2320        max1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "max2 (MaxPooling2D)             (None, 128, 16, 16)  0           conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 128, 256)     0           max2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 128, 32)      8224        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru1 (GRU)                      (None, 128, 512)     837120      dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru1_b (GRU)                    (None, 128, 512)     837120      dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 128, 512)     0           gru1[0][0]                       \n",
      "                                                                 gru1_b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru2 (GRU)                      (None, 128, 512)     1574400     add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gru2_b (GRU)                    (None, 128, 512)     1574400     add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128, 1024)    0           gru2[0][0]                       \n",
      "                                                                 gru2_b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 128, 54)      55350       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Activation)            (None, 128, 54)      0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         (None, 16)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           softmax[0][0]                    \n",
      "                                                                 the_labels[0][0]                 \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 4,889,094\n",
      "Trainable params: 4,889,094\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "on_train_begin\n",
      "Epoch 21/250\n",
      "on_epoch_begin\n",
      "234/400 [================>.............] - ETA: 48s - loss: 7.3812"
     ]
    }
   ],
   "source": [
    "run_name = datetime.datetime.now().strftime('%Y:%m:%d:%H:%M:%S')\n",
    "train(run_name, 0, 20, 128)\n",
    "# increase to wider images and start at epoch 20. The learned weights are reloaded\n",
    "img_gen.set_img_w(512)\n",
    "train(run_name, 20, 250, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "image_ocr.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
