{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "see README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline with generator_choice = \"GSQL\"\n",
    "200 interation got to 10.xxx something"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basedline with generator_choice =  \"German\" on July 31, 2019\n",
    "- Epoch 1/200  - loss: 22.5422 - val_loss: 44.3061\n",
    "- Epoch 2/200  - loss: 20.7822 - val_loss: 73.2133\n",
    "- Epoch 3/200  - loss: 20.5434 - val_loss: 20.1864\n",
    "- Epoch 4/200  - loss: 19.6281 - val_loss: 18.1584\n",
    "- Epoch 5/200  - loss: 18.3635 - val_loss: 17.9036\n",
    "- Epoch 6/200  - loss: 17.1934 - val_loss: 16.0855\n",
    "- Epoch 7/200  - loss: 16.2462 - val_loss: 14.7814\n",
    "- Epoch 8/200  - loss: 15.4579 - val_loss: 14.7433\n",
    "- Epoch 9/200  - loss: 14.7134 - val_loss: 13.2510\n",
    "- Epoch 10/200 - loss: 14.1979 - val_loss: 12.8388\n",
    "- Epoch 11/200 - loss: 13.7155 - val_loss: 12.4681\n",
    "- Epoch 12/200 - loss: 13.1744 - val_loss: 12.4697\n",
    "- Epoch 13/200 - loss: 12.8387 - val_loss: 12.2364\n",
    "- Epoch 14/200 - loss: 12.4855 - val_loss: 12.0318\n",
    "- Epoch 15/200 - loss: 12.1907 - val_loss: 11.1764\n",
    "- Epoch 16/200 - loss: 11.7321 - val_loss: 11.1777\n",
    "- Epoch 17/200 - loss: 11.5132 - val_loss: 11.0034\n",
    "- Epoch 18/200 - loss: 11.1854 - val_loss: 11.1696\n",
    "- Epoch 19/200 - loss: 11.0407 - val_loss: 10.7113\n",
    "- Epoch 20/200 - loss: 10.6401 - val_loss: 10.6150\n",
    "- Epoch 21/200 - loss: 10.4692 - val_loss: 10.4183\n",
    "- Epoch 22/200 - loss: 10.2791 - val_loss: 11.1796\n",
    "- Epoch 23/200 - loss: 9.9911 - val_loss: 10.4712\n",
    "- Epoch 24/200 - loss: 9.7312 - val_loss: 10.4887\n",
    "- Epoch 25/200 - loss: 9.4863 - val_loss: 10.3883\n",
    "- Epoch 26/200 - loss: 9.3771 - val_loss: 10.2179\n",
    "- Epoch 27/200 - loss: 9.0670 - val_loss: 10.1179\n",
    "- Epoch 28/200 - loss: 8.7698 - val_loss: 10.2423\n",
    "- Epoch 29/200 - loss: 8.6642 - val_loss: 10.3849\n",
    "- Epoch 30/200 - loss: 8.4662 - val_loss: 10.1517\n",
    "- Epoch 31/200 - loss: 8.2157 - val_loss: 10.0927\n",
    "- Epoch 32/200 - loss: 8.0640 - val_loss: 10.0808\n",
    "- Epoch 33/200 - loss: 7.8819 - val_loss: 10.0854\n",
    "- Epoch 34/200 - loss: 7.8030 - val_loss: 9.7388\n",
    "- Epoch 35/200 - loss: 7.5315 - val_loss: 10.1922\n",
    "- Epoch 36/200 - loss: 7.3847 - val_loss: 10.1470\n",
    "- Epoch 37/200 - loss: 7.3319 - val_loss: 10.0684\n",
    "- Epoch 38/200 - loss: 7.1130 - val_loss: 9.6827\n",
    "- Epoch 39/200 - loss: 7.0022 - val_loss: 10.3757\n",
    "- Epoch 40/200 - loss: 6.9908 - val_loss: 10.1155\n",
    "- Epoch 41/200 - loss: 6.6640 - val_loss: 10.3377\n",
    "- Epoch 42/200 - loss: 6.5059 - val_loss: 10.1471\n",
    "- Epoch 43/200 - loss: 6.6160 - val_loss: 9.5882\n",
    "- Epoch 44/200 - loss: 6.3216 - val_loss: 10.1955\n",
    "- Epoch 45/200 - loss: 6.2142 - val_loss: 10.1830\n",
    "- Epoch 46/200 - loss: 6.1089 - val_loss: 10.4291\n",
    "- Epoch 47/200 - loss: 6.0326 - val_loss: 10.4027\n",
    "- Epoch 48/200 - loss: 5.8245 - val_loss: 10.6347\n",
    "- Epoch 49/200 - loss: 5.9137 - val_loss: 10.6149\n",
    "- Epoch 50/200 - loss: 5.6554 - val_loss: 10.1823\n",
    "- Epoch 51/200 - loss: 5.6850 - val_loss: 9.9264\n",
    "- Epoch 52/200 - loss: 5.5150 - val_loss: 10.0987\n",
    "- Epoch 53/200 - loss: 5.4917 - val_loss: 9.8134\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7776,
     "status": "ok",
     "timestamp": 1526105952629,
     "user": {
      "displayName": "Chengwei Zhang",
      "photoUrl": "//lh5.googleusercontent.com/-FK2ckwmh6mM/AAAAAAAAAAI/AAAAAAAAPLw/SX9b1QAzJ5g/s50-c-k-no/photo.jpg",
      "userId": "114808171854651597062"
     },
     "user_tz": -480
    },
    "id": "1wsDPx682A7H",
    "outputId": "88ee055c-b674-4a81-c869-8401551f09d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "\n",
    "\n",
    "import datetime\n",
    "\n",
    "import editdistance\n",
    "import numpy as np\n",
    "\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.layers import Reshape, Lambda\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing import image\n",
    "import keras.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Xnj6R_OE3yMl"
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = 'image_ocr'\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mM0eHsJO5bP6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code under development ...\n",
      "click debug in visual studio code\n",
      "code under development ...\n",
      "click debug in visual studio code\n",
      "WAITING FOR DEBUGGER\n",
      "2\n",
      "code under development ...\n",
      "click debug in visual studio code\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Text_Image is the original generator. It creates images programaticaly. \n",
    "# Script_Image takes handwritten words from the IAM database.\n",
    "generator_choice =  \"GSQL\" #\"German\" # \"Script_Image\" # \"German\" # \"Text_Image\" #\n",
    "import generator_text_image as GTI\n",
    "import generator_iam_words as IAM\n",
    "import generator_german_words as GER\n",
    "import generator_SQL_words as GSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "HBLwtxF35f0c"
   },
   "outputs": [],
   "source": [
    "import ctc_drop_first_2\n",
    "import cnn_rnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "YOAsKfXm5jP1"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# For a real OCR application, this should be beam search with a dictionary\n",
    "# and language model.  For this example, best path is sufficient.\n",
    "\n",
    "def decode_batch(test_func, word_batch):\n",
    "    out = test_func([word_batch])[0]\n",
    "    ret = []\n",
    "    for j in range(out.shape[0]):\n",
    "        out_best = list(np.argmax(out[j, 2:], 1))\n",
    "        out_best = [k for k, g in itertools.groupby(out_best)]\n",
    "        if generator_choice ==  \"Script_Image\":\n",
    "            outstr = IAM.labels_to_text(out_best)\n",
    "        elif generator_choice == \"German\":\n",
    "            outstr = GER.labels_to_text(out_best)\n",
    "        elif generator_choice == \"GSQL\":\n",
    "            outstr = GSQL.labels_to_text(out_best)\n",
    "        elif generator_choice == \"Text_Image\":\n",
    "            outstr = GTI.labels_to_text(out_best)\n",
    "        else:\n",
    "            assert(False)\n",
    "        ret.append(outstr)\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KcGtIsLF5leW"
   },
   "outputs": [],
   "source": [
    "class VizCallback(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, run_name, test_func, text_img_gen, num_display_words=6):\n",
    "        self.test_func = test_func\n",
    "        self.output_dir = os.path.join(\n",
    "            OUTPUT_DIR, run_name)\n",
    "        self.text_img_gen = text_img_gen\n",
    "        self.num_display_words = num_display_words\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "    def show_edit_distance(self, num):\n",
    "        num_left = num\n",
    "        mean_norm_ed = 0.0\n",
    "        mean_ed = 0.0\n",
    "        while num_left > 0:\n",
    "            word_batch = next(self.text_img_gen)[0]\n",
    "            num_proc = min(word_batch['the_input'].shape[0], num_left)\n",
    "            decoded_res = decode_batch(self.test_func, word_batch['the_input'][0:num_proc])\n",
    "            for j in range(num_proc):\n",
    "                edit_dist = editdistance.eval(decoded_res[j], word_batch['source_str'][j])\n",
    "                mean_ed += float(edit_dist)\n",
    "                mean_norm_ed += float(edit_dist) / len(word_batch['source_str'][j])\n",
    "            num_left -= num_proc\n",
    "        mean_norm_ed = mean_norm_ed / num\n",
    "        mean_ed = mean_ed / num\n",
    "        print('\\nOut of %d samples:  Mean edit distance: %.3f Mean normalized edit distance: %0.3f'\n",
    "              % (num, mean_ed, mean_norm_ed))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.model.save_weights(os.path.join(self.output_dir, 'weights%02d.h5' % (epoch)))\n",
    "        self.show_edit_distance(256)\n",
    "        word_batch = next(self.text_img_gen)[0]\n",
    "        res = decode_batch(self.test_func, word_batch['the_input'][0:self.num_display_words])\n",
    "        if word_batch['the_input'][0].shape[0] < 256:\n",
    "            cols = 2\n",
    "        else:\n",
    "            cols = 1\n",
    "        for i in range(self.num_display_words):\n",
    "            plt.subplot(self.num_display_words // cols, cols, i + 1)\n",
    "            if K.image_data_format() == 'channels_first':\n",
    "                the_input = word_batch['the_input'][i, 0, :, :]\n",
    "            else:\n",
    "                the_input = word_batch['the_input'][i, :, :, 0]\n",
    "            plt.imshow(the_input.T, cmap='Greys_r')\n",
    "            plt.xlabel('Truth = \\'%s\\'\\nDecoded = \\'%s\\'' % (word_batch['source_str'][i], res[i]))\n",
    "        fig = pylab.gcf()\n",
    "        fig.set_size_inches(10, 13)\n",
    "        plt.savefig(os.path.join(self.output_dir, 'e%02d.png' % (epoch)))\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "eC4vEfcQ5oOq"
   },
   "outputs": [],
   "source": [
    "img_gen = None\n",
    "def train(run_name, start_epoch, stop_epoch, img_w):\n",
    "    global img_gen\n",
    "    img_h = 64\n",
    "    \n",
    "    # Input Parameters\n",
    "\n",
    "    words_per_epoch = 16000\n",
    "    val_split = 0.2\n",
    "    \n",
    "    # 16000 * 0.2 = 3200\n",
    "    val_words = int(words_per_epoch * (val_split))\n",
    "\n",
    "    # Network parameters\n",
    "    minibatch_size = 32\n",
    "    \n",
    "    \n",
    "    # (16000 - 3200) // 32 = 400\n",
    "    steps_per_epoch = (words_per_epoch - val_words) // minibatch_size\n",
    "\n",
    "\n",
    "    if generator_choice == \"Text_Image\":\n",
    "        fdir = os.path.dirname(get_file('wordlists.tgz',\n",
    "                                        origin='http://www.mythic-ai.com/datasets/wordlists.tgz', untar=True))\n",
    "\n",
    "        img_gen = GTI.TextImageGenerator(monogram_file=os.path.join(fdir, 'wordlist_mono_clean.txt'),\n",
    "                                     bigram_file=os.path.join(fdir, 'wordlist_bi_clean.txt'),\n",
    "                                     minibatch_size=minibatch_size,\n",
    "                                     img_w=img_w,\n",
    "                                     img_h=img_h,\n",
    "                                     downsample_factor=(pool_size ** 2),\n",
    "                                     val_split=words_per_epoch - val_words\n",
    "                                     )\n",
    "    elif generator_choice == \"Script_Image\":\n",
    "        img_gen = IAM.IAM_Word_Generator(minibatch_size = 32, img_w = img_w, img_h = img_h, downsample_factor=4, absolute_max_string_len=16)\n",
    "    elif generator_choice == \"German\":\n",
    "        img_gen = GER.German_Word_Generator(minibatch_size = 32, img_w = img_w, img_h = img_h, downsample_factor=4, absolute_max_string_len=16)\n",
    "    elif generator_choice == \"GSQL\":\n",
    "        img_gen = GSQL.German_Word_Generator(minibatch_size = 32, img_w = img_w, img_h = img_h, downsample_factor=4, absolute_max_string_len=16)\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "    model, model_p, input_data, y_pred = cnn_rnn_model.make_model(img_w, img_h, img_gen.get_output_size(), img_gen.absolute_max_string_len)\n",
    "    \n",
    "    model_p.summary() # print summary of model before added ctc\n",
    "    model.summary() # print summary of model\n",
    "    \n",
    "    from keras.utils import plot_model\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    from IPython.display import Image\n",
    "    Image(filename='model.png')\n",
    "\n",
    "    # clipnorm seems to speeds up convergence\n",
    "    sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
    "    \n",
    "    # the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n",
    "    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=sgd)\n",
    "    if start_epoch > 0:\n",
    "        weight_file = os.path.join(OUTPUT_DIR, os.path.join(run_name, 'weights%02d.h5' % (start_epoch - 1)))\n",
    "        model.load_weights(weight_file)\n",
    "    # captures output of softmax so we can decode the output during visualization\n",
    "    test_func = K.function([input_data], [y_pred])\n",
    "\n",
    "    viz_cb = VizCallback(run_name, test_func, img_gen.next_val())\n",
    "\n",
    "    model.fit_generator(generator=img_gen.next_train(),\n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        epochs=stop_epoch,\n",
    "                        validation_data=img_gen.next_val(),\n",
    "                        validation_steps=val_words // minibatch_size,\n",
    "                        callbacks=[viz_cb, img_gen],\n",
    "                        initial_epoch=start_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "13kO-MxI7XWb"
   },
   "source": [
    "### Start the training\n",
    "\n",
    "1080ti run times: \n",
    "- 12 minutes for 20/20 epoch\n",
    "- 25 minutes for 25/25 epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2094
    },
    "colab_type": "code",
    "id": "YRGglF5L5qzn",
    "outputId": "541e795b-80d1-49e6-9c9f-e2014690f4a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Tensor(\"the_labels:0\", shape=(?, 16), dtype=float32)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4249: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4229: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          (None, 128, 64, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 128, 64, 16)  160         the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max1 (MaxPooling2D)             (None, 64, 32, 16)   0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 64, 32, 16)   2320        max1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "max2 (MaxPooling2D)             (None, 32, 16, 16)   0           conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 32, 256)      0           max2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 32, 32)       8224        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru1 (GRU)                      (None, 32, 512)      837120      dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru1_b (GRU)                    (None, 32, 512)      837120      dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 512)      0           gru1[0][0]                       \n",
      "                                                                 gru1_b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru2 (GRU)                      (None, 32, 512)      1574400     add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gru2_b (GRU)                    (None, 32, 512)      1574400     add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 1024)     0           gru2[0][0]                       \n",
      "                                                                 gru2_b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 32, 68)       69700       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Activation)            (None, 32, 68)       0           dense2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 4,903,444\n",
      "Trainable params: 4,903,444\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          (None, 128, 64, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 128, 64, 16)  160         the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max1 (MaxPooling2D)             (None, 64, 32, 16)   0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 64, 32, 16)   2320        max1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "max2 (MaxPooling2D)             (None, 32, 16, 16)   0           conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 32, 256)      0           max2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 32, 32)       8224        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru1 (GRU)                      (None, 32, 512)      837120      dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru1_b (GRU)                    (None, 32, 512)      837120      dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 512)      0           gru1[0][0]                       \n",
      "                                                                 gru1_b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru2 (GRU)                      (None, 32, 512)      1574400     add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gru2_b (GRU)                    (None, 32, 512)      1574400     add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 1024)     0           gru2[0][0]                       \n",
      "                                                                 gru2_b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 32, 68)       69700       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Activation)            (None, 32, 68)       0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         (None, 16)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           softmax[0][0]                    \n",
      "                                                                 the_labels[0][0]                 \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 4,903,444\n",
      "Trainable params: 4,903,444\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/200\n",
      "400/400 [==============================] - 81s 203ms/step - loss: 21.3263 - val_loss: 18.0781\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 4.949 Mean normalized edit distance: 0.994\n",
      "Epoch 2/200\n",
      "400/400 [==============================] - 78s 195ms/step - loss: 17.9209 - val_loss: 17.2978\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 4.605 Mean normalized edit distance: 0.892\n",
      "Epoch 3/200\n",
      "400/400 [==============================] - 78s 195ms/step - loss: 17.0103 - val_loss: 16.0944\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 4.391 Mean normalized edit distance: 0.873\n",
      "Epoch 4/200\n",
      "400/400 [==============================] - 77s 193ms/step - loss: 16.0455 - val_loss: 15.7953\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 5.039 Mean normalized edit distance: 0.973\n",
      "Epoch 5/200\n",
      "400/400 [==============================] - 79s 196ms/step - loss: 15.3872 - val_loss: 14.9153\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 3.977 Mean normalized edit distance: 0.788\n",
      "Epoch 6/200\n",
      "400/400 [==============================] - 78s 196ms/step - loss: 14.7822 - val_loss: 14.3283\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 3.930 Mean normalized edit distance: 0.776\n",
      "Epoch 7/200\n",
      "400/400 [==============================] - 78s 194ms/step - loss: 14.1549 - val_loss: 14.3040\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 4.418 Mean normalized edit distance: 0.855\n",
      "Epoch 8/200\n",
      "400/400 [==============================] - 78s 195ms/step - loss: 13.8425 - val_loss: 13.5541\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 3.906 Mean normalized edit distance: 0.743\n",
      "Epoch 9/200\n",
      "400/400 [==============================] - 78s 196ms/step - loss: 13.6181 - val_loss: 13.1735\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 3.895 Mean normalized edit distance: 0.755\n",
      "Epoch 10/200\n",
      "400/400 [==============================] - 78s 194ms/step - loss: 13.2209 - val_loss: 12.6179\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 3.957 Mean normalized edit distance: 0.737\n",
      "Epoch 11/200\n",
      "400/400 [==============================] - 77s 193ms/step - loss: 12.9493 - val_loss: 12.6486\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 3.941 Mean normalized edit distance: 0.754\n",
      "Epoch 12/200\n",
      "400/400 [==============================] - 78s 195ms/step - loss: 12.6268 - val_loss: 11.9722\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 3.445 Mean normalized edit distance: 0.670\n",
      "Epoch 13/200\n",
      "400/400 [==============================] - 77s 193ms/step - loss: 12.0823 - val_loss: 11.7014\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 3.656 Mean normalized edit distance: 0.665\n",
      "Epoch 14/200\n",
      "400/400 [==============================] - 76s 191ms/step - loss: 11.9440 - val_loss: 11.8145\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 3.344 Mean normalized edit distance: 0.630\n",
      "Epoch 15/200\n",
      "400/400 [==============================] - 78s 194ms/step - loss: 11.7599 - val_loss: 11.3336\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 3.332 Mean normalized edit distance: 0.599\n",
      "Epoch 16/200\n",
      "400/400 [==============================] - 77s 192ms/step - loss: 11.3730 - val_loss: 11.2386\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 3.254 Mean normalized edit distance: 0.583\n",
      "Epoch 17/200\n",
      "400/400 [==============================] - 77s 193ms/step - loss: 11.1966 - val_loss: 11.3916\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 3.172 Mean normalized edit distance: 0.620\n",
      "Epoch 18/200\n",
      "400/400 [==============================] - 77s 193ms/step - loss: 11.1887 - val_loss: 11.0063\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 3.211 Mean normalized edit distance: 0.590\n",
      "Epoch 19/200\n",
      "400/400 [==============================] - 77s 194ms/step - loss: 10.8901 - val_loss: 10.6043\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 3.051 Mean normalized edit distance: 0.567\n",
      "Epoch 20/200\n",
      "400/400 [==============================] - 77s 193ms/step - loss: 10.7167 - val_loss: 10.7657\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 3.047 Mean normalized edit distance: 0.565\n",
      "Epoch 21/200\n",
      "400/400 [==============================] - 77s 191ms/step - loss: 10.5931 - val_loss: 10.4786\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 3.441 Mean normalized edit distance: 0.602\n",
      "Epoch 22/200\n",
      "400/400 [==============================] - 77s 194ms/step - loss: 10.3041 - val_loss: 10.5413\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 3.102 Mean normalized edit distance: 0.570\n",
      "Epoch 23/200\n",
      "400/400 [==============================] - 78s 195ms/step - loss: 10.3066 - val_loss: 10.5094\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 3.297 Mean normalized edit distance: 0.594\n",
      "Epoch 24/200\n",
      "400/400 [==============================] - 77s 193ms/step - loss: 10.1073 - val_loss: 10.3886\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.965 Mean normalized edit distance: 0.566\n",
      "Epoch 25/200\n",
      "400/400 [==============================] - 77s 192ms/step - loss: 9.8969 - val_loss: 10.3522\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 3.121 Mean normalized edit distance: 0.603\n",
      "Epoch 26/200\n",
      "400/400 [==============================] - 78s 194ms/step - loss: 9.8664 - val_loss: 10.1504\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.887 Mean normalized edit distance: 0.535\n",
      "Epoch 27/200\n",
      "400/400 [==============================] - 78s 195ms/step - loss: 9.8179 - val_loss: 10.1699\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.848 Mean normalized edit distance: 0.523\n",
      "Epoch 28/200\n",
      "400/400 [==============================] - 77s 193ms/step - loss: 9.6791 - val_loss: 10.0967\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 3.039 Mean normalized edit distance: 0.538\n",
      "Epoch 29/200\n",
      "400/400 [==============================] - 77s 194ms/step - loss: 9.5504 - val_loss: 9.5369\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.660 Mean normalized edit distance: 0.508\n",
      "Epoch 30/200\n",
      "400/400 [==============================] - 77s 193ms/step - loss: 9.3400 - val_loss: 10.6250\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.965 Mean normalized edit distance: 0.558\n",
      "Epoch 31/200\n",
      "400/400 [==============================] - 77s 192ms/step - loss: 9.3127 - val_loss: 10.1111\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 3.074 Mean normalized edit distance: 0.562\n",
      "Epoch 32/200\n",
      "400/400 [==============================] - 78s 194ms/step - loss: 9.2587 - val_loss: 9.5720\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.789 Mean normalized edit distance: 0.492\n",
      "Epoch 33/200\n",
      "400/400 [==============================] - 77s 192ms/step - loss: 9.0398 - val_loss: 9.5223\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.820 Mean normalized edit distance: 0.527\n",
      "Epoch 34/200\n",
      "400/400 [==============================] - 78s 196ms/step - loss: 9.0691 - val_loss: 9.5442\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.922 Mean normalized edit distance: 0.543\n",
      "Epoch 35/200\n",
      "400/400 [==============================] - 77s 192ms/step - loss: 9.0563 - val_loss: 9.3956\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.859 Mean normalized edit distance: 0.509\n",
      "Epoch 36/200\n",
      "400/400 [==============================] - 77s 192ms/step - loss: 8.9276 - val_loss: 9.4033\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.867 Mean normalized edit distance: 0.524\n",
      "Epoch 37/200\n",
      "400/400 [==============================] - 77s 193ms/step - loss: 8.7738 - val_loss: 9.8069\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.641 Mean normalized edit distance: 0.505\n",
      "Epoch 38/200\n",
      "400/400 [==============================] - 77s 191ms/step - loss: 8.6510 - val_loss: 9.0654\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.738 Mean normalized edit distance: 0.525\n",
      "Epoch 39/200\n",
      "400/400 [==============================] - 77s 193ms/step - loss: 8.5777 - val_loss: 9.5884\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.984 Mean normalized edit distance: 0.505\n",
      "Epoch 40/200\n",
      "400/400 [==============================] - 77s 192ms/step - loss: 8.5454 - val_loss: 10.1613\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.949 Mean normalized edit distance: 0.554\n",
      "Epoch 41/200\n",
      "400/400 [==============================] - 77s 192ms/step - loss: 8.5213 - val_loss: 9.2575\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.457 Mean normalized edit distance: 0.484\n",
      "Epoch 42/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 78s 194ms/step - loss: 8.2371 - val_loss: 9.2421\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.645 Mean normalized edit distance: 0.466\n",
      "Epoch 43/200\n",
      "400/400 [==============================] - 77s 192ms/step - loss: 8.2734 - val_loss: 9.0836\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.590 Mean normalized edit distance: 0.471\n",
      "Epoch 44/200\n",
      "400/400 [==============================] - 77s 194ms/step - loss: 8.3937 - val_loss: 9.2969\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.539 Mean normalized edit distance: 0.481\n",
      "Epoch 45/200\n",
      "400/400 [==============================] - 78s 195ms/step - loss: 8.2088 - val_loss: 9.3628\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.859 Mean normalized edit distance: 0.500\n",
      "Epoch 46/200\n",
      "400/400 [==============================] - 76s 191ms/step - loss: 7.9955 - val_loss: 9.6505\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.766 Mean normalized edit distance: 0.508\n",
      "Epoch 47/200\n",
      "400/400 [==============================] - 77s 192ms/step - loss: 8.0891 - val_loss: 9.2973\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.824 Mean normalized edit distance: 0.491\n",
      "Epoch 48/200\n",
      "400/400 [==============================] - 78s 194ms/step - loss: 7.8689 - val_loss: 9.2559\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.512 Mean normalized edit distance: 0.469\n",
      "Epoch 49/200\n",
      "400/400 [==============================] - 77s 193ms/step - loss: 7.7595 - val_loss: 9.5660\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 3.098 Mean normalized edit distance: 0.497\n",
      "Epoch 50/200\n",
      "400/400 [==============================] - 77s 194ms/step - loss: 7.9304 - val_loss: 9.4826\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.438 Mean normalized edit distance: 0.466\n",
      "Epoch 51/200\n",
      "400/400 [==============================] - 77s 193ms/step - loss: 7.7953 - val_loss: 9.4996\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.570 Mean normalized edit distance: 0.448\n",
      "Epoch 52/200\n",
      "400/400 [==============================] - 77s 193ms/step - loss: 7.8639 - val_loss: 9.2452\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.848 Mean normalized edit distance: 0.493\n",
      "Epoch 53/200\n",
      "400/400 [==============================] - 77s 193ms/step - loss: 7.6258 - val_loss: 9.4537\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.719 Mean normalized edit distance: 0.506\n",
      "Epoch 54/200\n",
      "400/400 [==============================] - 77s 192ms/step - loss: 7.5889 - val_loss: 9.2505\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.562 Mean normalized edit distance: 0.443\n",
      "Epoch 55/200\n",
      "400/400 [==============================] - 76s 191ms/step - loss: 7.4809 - val_loss: 8.9996\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.625 Mean normalized edit distance: 0.491\n",
      "Epoch 56/200\n",
      "400/400 [==============================] - 78s 194ms/step - loss: 7.3478 - val_loss: 9.0809\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.723 Mean normalized edit distance: 0.511\n",
      "Epoch 57/200\n",
      "400/400 [==============================] - 77s 191ms/step - loss: 7.3639 - val_loss: 9.1869\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.680 Mean normalized edit distance: 0.497\n",
      "Epoch 58/200\n",
      "400/400 [==============================] - 78s 195ms/step - loss: 7.3064 - val_loss: 9.0530\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.684 Mean normalized edit distance: 0.478\n",
      "Epoch 59/200\n",
      "400/400 [==============================] - 79s 196ms/step - loss: 7.3026 - val_loss: 9.4617\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.484 Mean normalized edit distance: 0.464\n",
      "Epoch 60/200\n",
      "400/400 [==============================] - 78s 195ms/step - loss: 7.0789 - val_loss: 9.2646\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.367 Mean normalized edit distance: 0.441\n",
      "Epoch 61/200\n",
      "400/400 [==============================] - 79s 198ms/step - loss: 7.2381 - val_loss: 9.1922\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.586 Mean normalized edit distance: 0.464\n",
      "Epoch 62/200\n",
      "400/400 [==============================] - 79s 197ms/step - loss: 7.1261 - val_loss: 9.2609\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.699 Mean normalized edit distance: 0.481\n",
      "Epoch 63/200\n",
      "400/400 [==============================] - 79s 196ms/step - loss: 7.0780 - val_loss: 9.1049\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.488 Mean normalized edit distance: 0.457\n",
      "Epoch 64/200\n",
      "400/400 [==============================] - 78s 195ms/step - loss: 6.9134 - val_loss: 9.3022\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.539 Mean normalized edit distance: 0.462\n",
      "Epoch 65/200\n",
      "400/400 [==============================] - 77s 193ms/step - loss: 6.8123 - val_loss: 9.0191\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.617 Mean normalized edit distance: 0.476\n",
      "Epoch 66/200\n",
      "400/400 [==============================] - 77s 193ms/step - loss: 6.8600 - val_loss: 9.3807\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.691 Mean normalized edit distance: 0.476\n",
      "Epoch 67/200\n",
      "400/400 [==============================] - 77s 193ms/step - loss: 6.9748 - val_loss: 9.2329\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.496 Mean normalized edit distance: 0.487\n",
      "Epoch 68/200\n",
      "400/400 [==============================] - 77s 192ms/step - loss: 6.7574 - val_loss: 9.3437\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.773 Mean normalized edit distance: 0.472\n",
      "Epoch 69/200\n",
      "400/400 [==============================] - 78s 195ms/step - loss: 6.9081 - val_loss: 9.3757\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.734 Mean normalized edit distance: 0.470\n",
      "Epoch 70/200\n",
      "400/400 [==============================] - 77s 193ms/step - loss: 6.6765 - val_loss: 9.3175\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.434 Mean normalized edit distance: 0.464\n",
      "Epoch 71/200\n",
      "400/400 [==============================] - 77s 193ms/step - loss: 6.7674 - val_loss: 9.1107\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.441 Mean normalized edit distance: 0.447\n",
      "Epoch 72/200\n",
      "400/400 [==============================] - 78s 195ms/step - loss: 6.6251 - val_loss: 9.5513\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.426 Mean normalized edit distance: 0.472\n",
      "Epoch 73/200\n",
      "400/400 [==============================] - 77s 192ms/step - loss: 6.5644 - val_loss: 9.1013\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.387 Mean normalized edit distance: 0.487\n",
      "Epoch 74/200\n",
      "400/400 [==============================] - 77s 192ms/step - loss: 6.5412 - val_loss: 9.3797\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.398 Mean normalized edit distance: 0.434\n",
      "Epoch 75/200\n",
      "400/400 [==============================] - 78s 194ms/step - loss: 6.6324 - val_loss: 9.4525\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.457 Mean normalized edit distance: 0.437\n",
      "Epoch 76/200\n",
      "400/400 [==============================] - 77s 193ms/step - loss: 6.3438 - val_loss: 9.2569\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.633 Mean normalized edit distance: 0.468\n",
      "Epoch 77/200\n",
      "400/400 [==============================] - 76s 191ms/step - loss: 6.2778 - val_loss: 9.2793\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.461 Mean normalized edit distance: 0.467\n",
      "Epoch 78/200\n",
      "400/400 [==============================] - 78s 196ms/step - loss: 6.4089 - val_loss: 9.6889\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.738 Mean normalized edit distance: 0.495\n",
      "Epoch 79/200\n",
      "400/400 [==============================] - 78s 195ms/step - loss: 6.2503 - val_loss: 9.4869\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.516 Mean normalized edit distance: 0.475\n",
      "Epoch 80/200\n",
      "400/400 [==============================] - 77s 194ms/step - loss: 6.2285 - val_loss: 9.9914\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.773 Mean normalized edit distance: 0.474\n",
      "Epoch 81/200\n",
      "400/400 [==============================] - 78s 195ms/step - loss: 6.4000 - val_loss: 9.5331\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.676 Mean normalized edit distance: 0.444\n",
      "Epoch 82/200\n",
      "400/400 [==============================] - 79s 197ms/step - loss: 6.4948 - val_loss: 9.4589\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.520 Mean normalized edit distance: 0.473\n",
      "Epoch 83/200\n",
      "400/400 [==============================] - 79s 199ms/step - loss: 6.2572 - val_loss: 9.4705\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.629 Mean normalized edit distance: 0.466\n",
      "Epoch 84/200\n",
      "400/400 [==============================] - 80s 199ms/step - loss: 6.3560 - val_loss: 9.2091\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.492 Mean normalized edit distance: 0.444\n",
      "Epoch 85/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 79s 197ms/step - loss: 6.3884 - val_loss: 9.8622\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.418 Mean normalized edit distance: 0.438\n",
      "Epoch 86/200\n",
      "380/400 [===========================>..] - ETA: 3s - loss: 6.2886"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-48444e42ee6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## run_name = datetime.datetime.now().strftime('%Y:%m:%d:%H:%M:%S')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrun_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y:%m:%d:%H:%M:%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# increase to wider images and start at epoch 20. The learned weights are reloaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimg_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_img_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-39c5f2444f6b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(run_name, start_epoch, stop_epoch, img_w)\u001b[0m\n\u001b[1;32m     70\u001b[0m                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_words\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mviz_cb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_gen\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                         initial_epoch=start_epoch)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## run_name = datetime.datetime.now().strftime('%Y:%m:%d:%H:%M:%S')\n",
    "run_name = datetime.datetime.now().strftime('%Y:%m:%d:%H:%M:%S')\n",
    "train(run_name, 0, 200, 128)\n",
    "# increase to wider images and start at epoch 20. The learned weights are reloaded\n",
    "img_gen.set_img_w(512)\n",
    "train(run_name, 20, 250, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "image_ocr.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
